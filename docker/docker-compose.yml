version: "3.8"

x-airflow-common: &airflow-common
  image: apache/airflow:2.9.3
  env_file:
    - ../.env               # aquí pondrás NEWSAPI_KEY si lo usas, y AIRFLOW_CONN_SUPABASE_DB (opcional)
  environment:
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__LOAD_EXAMPLES: "False"
    AIRFLOW__CORE__FERNET_KEY: "${AIRFLOW_FERNET_KEY}"
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
    AIRFLOW__LOGGING__LOGGING_LEVEL: INFO
    AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 30
    AIRFLOW_UID: "${AIRFLOW_UID:-50000}"
  volumes:
    - ../dags:/opt/airflow/dags
    - ../plugins:/opt/airflow/plugins
    - ../requirements-airflow.txt:/requirements.txt
    - ..:/opt/airflow/app           # Monto TU repo para poder importar src.*
  user: "${AIRFLOW_UID:-50000}:0"
  working_dir: /opt/airflow
  command: >
    bash -lc "pip install -r /requirements.txt &&
              airflow db migrate &&
              airflow users create -u admin -p admin -r Admin -e admin@example.com -f Admin -l User &&
              airflow webserver"

services:
  airflow-webserver:
    <<: *airflow-common
    ports:
      - "8080:8080"
    environment:
      PYTHONPATH: "/opt/airflow/app"   # para importar src.*

  airflow-scheduler:
    <<: *airflow-common
    command: >
      bash -lc "pip install -r /requirements.txt && airflow scheduler"
    environment:
      PYTHONPATH: "/opt/airflow/app"

  airflow-worker:
    <<: *airflow-common
    command: >
      bash -lc "pip install -r /requirements.txt && airflow celery worker"
    environment:
      PYTHONPATH: "/opt/airflow/app"

  redis:
    image: redis:7
    ports:
      - "6379:6379"
