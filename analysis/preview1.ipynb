{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preview ETL — Análisis exploratorio y diseño del pipeline\n",
        "\n",
        "Este cuaderno documenta el **análisis exploratorio** de los datos provenientes de NewsAPI y cómo se diseñó la **ETL (Extract → Transform → Load)** del proyecto. \n",
        "\n",
        "**Objetivos:**\n",
        "- Validar la **extracción** desde la API con queries reales.\n",
        "- Entender cómo vienen los **datos crudos** (nulos, duplicados, formatos, columnas).\n",
        "- Aplicar las **transformaciones** usadas en el proyecto (normalización, limpieza, filtros, deduplicación).\n",
        "- Simular la **carga** a base de datos y revisar la política de *upsert* por `url_hash`.\n",
        "\n",
        "> **Nota:** Este notebook está pensado como documento demostrativo. No publica credenciales ni las requiere para ejecutarse en modo \"preview\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Parámetros y configuración\n",
        "Leemos variables de entorno (sin exponerlas en el cuaderno) y preparamos imports. No importamos el módulo de `settings` para evitar fallas si faltan secrets; en su lugar usamos `os.getenv` de forma segura.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os, sys, json, pandas as pd\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "# Asegurar que el proyecto esté en el path\n",
        "PROJECT_ROOT = '.'\n",
        "if PROJECT_ROOT not in sys.path:\n",
        "    sys.path.append(PROJECT_ROOT)\n",
        "\n",
        "# Lectura segura de variables de entorno\n",
        "NEWSAPI_KEY = os.getenv('NEWSAPI_KEY')\n",
        "API_URL = os.getenv('API_URL', 'https://newsapi.org/v2/everything')\n",
        "DATABASE_URL = os.getenv('DATABASE_URL')  # puede ser None durante el preview\n",
        "\n",
        "print('API_URL =', API_URL)\n",
        "print('NEWSAPI_KEY presente =', bool(NEWSAPI_KEY))\n",
        "print('DATABASE_URL presente =', bool(DATABASE_URL))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Construcción de la query (desde BBDD)\n",
        "En el pipeline de producción, las *keywords* se obtienen desde BBDD mediante `build_q_from_db(engine)`. Aquí intentamos usarla si hay `DATABASE_URL`; si no, usamos un **fallback** estático para la demo.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "queries = None\n",
        "try:\n",
        "    from src.repositories.db import init_engine\n",
        "    from src.utils.query_builder import build_q_from_db\n",
        "    \n",
        "    if DATABASE_URL:\n",
        "        engine = init_engine(DATABASE_URL)\n",
        "        queries = build_q_from_db(engine)\n",
        "        print('Queries desde BBDD:', queries)\n",
        "    else:\n",
        "        print('DATABASE_URL no disponible: usaremos fallback local de queries.')\n",
        "except Exception as e:\n",
        "    print('No se pudo construir queries desde BBDD:', str(e))\n",
        "\n",
        "if not queries:\n",
        "    # Fallback para la exploración local\n",
        "    queries = '(artificial intelligence OR machine learning) AND (marketing OR growth)'\n",
        "    print('Queries (fallback):', queries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Extracción de datos (Extract)\n",
        "Usamos el servicio `fetch_ai_marketing_news(api_url, params)` que centraliza el acceso a la API y normaliza la respuesta a un `DataFrame` (cuando hay artículos). \n",
        "\n",
        "**Campos típicos de la respuesta cruda** (a través de `pd.json_normalize`):\n",
        "- `author`, `title`, `description`, `url`, `urlToImage`, `publishedAt`, `content`\n",
        "- `source.id` → `source_id`, `source.name` → `source_name`\n",
        "\n",
        "> Para evitar exponer la API key, esta celda **no se ejecutará automáticamente** aquí. Puedes activar la extracción definiendo `DO_EXTRACT=True` y asegurándote de tener `NEWSAPI_KEY` en tu entorno.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from typing import Optional, Tuple\n",
        "from pprint import pprint\n",
        "\n",
        "DO_EXTRACT = False  # cambia a True si quieres probar la llamada real (requiere NEWSAPI_KEY)\n",
        "\n",
        "df_raw: Optional[pd.DataFrame] = None\n",
        "meta: dict = {}\n",
        "\n",
        "if DO_EXTRACT:\n",
        "    from src.services.fetch_service import fetch_ai_marketing_news\n",
        "    \n",
        "    now = datetime.now(timezone.utc)\n",
        "    days_back = 7\n",
        "    frm = (now - timedelta(days=days_back)).isoformat(timespec='seconds')\n",
        "    to  = now.isoformat(timespec='seconds')\n",
        "    \n",
        "    params = {\n",
        "        'apiKey': NEWSAPI_KEY,\n",
        "        'q': queries,\n",
        "        'page': 1,\n",
        "        'pageSize': 100,\n",
        "        'sortBy': 'publishedAt',\n",
        "        'from': frm,\n",
        "        'to': to,\n",
        "        'language': 'en'\n",
        "    }\n",
        "    \n",
        "    df_raw, meta = fetch_ai_marketing_news(api_url=API_URL, params=params)\n",
        "    print('Metadatos de la respuesta:')\n",
        "    pprint(meta)\n",
        "    \n",
        "    if df_raw is not None and not df_raw.empty:\n",
        "        display(df_raw.head(3))\n",
        "        print('\\nColumnas crudas:', list(df_raw.columns))\n",
        "    else:\n",
        "        print('Sin artículos en la respuesta o DataFrame vacío.')\n",
        "else:\n",
        "    print('Extracción desactivada (DO_EXTRACT=False).')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Transformación de datos (Transform)\n",
        "Aplicamos las transformaciones del proyecto:\n",
        "- `clean_raw_data`: normaliza columnas, rellena valores por defecto, convierte `publishedAt` a UTC y genera `url_hash` a partir de la URL normalizada.\n",
        "- `filter_by_min_length`: descarta artículos con contenido demasiado corto (según `content` + `\"[+XXXX chars]\"`).\n",
        "\n",
        "Mostramos comparación **antes vs después** y el esquema resultante.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from src.services.clean_service import clean_raw_data, filter_by_min_length\n",
        "\n",
        "# Para poder ejecutar la sección sin API, sintetizamos un pequeño df_raw si no hay extracción\n",
        "if 'df_raw' not in globals() or df_raw is None:\n",
        "    df_raw = pd.DataFrame([\n",
        "        {\n",
        "            'author': 'Alice', 'title': 'AI boosts marketing', 'description': 'Short desc',\n",
        "            'url': 'https://example.com/article?utm_source=test', 'urlToImage': None,\n",
        "            'publishedAt': datetime.now(timezone.utc).isoformat(), 'content': 'Some content [+1200 chars]',\n",
        "            'source_id': 'ex', 'source_name': 'Example Source'\n",
        "        },\n",
        "        {\n",
        "            'author': None, 'title': 'ML in growth', 'description': 'Desc 2',\n",
        "            'url': 'https://example.com/article', 'urlToImage': 'https://example.com/img.jpg',\n",
        "            'publishedAt': datetime.now(timezone.utc).isoformat(), 'content': 'Tiny',\n",
        "            'source_id': None, 'source_name': 'Example Source'\n",
        "        }\n",
        "    ])\n",
        "\n",
        "n_before = 0 if df_raw is None else len(df_raw)\n",
        "df_clean = clean_raw_data(df_raw)\n",
        "df_clean = filter_by_min_length(df_clean, min_total_chars=800)\n",
        "n_after = 0 if df_clean is None else len(df_clean)\n",
        "\n",
        "print(f'Registros antes: {n_before}  |  después de limpieza+filtro: {n_after}')\n",
        "display(df_clean.head(3))\n",
        "print('\\nColumnas finales:', list(df_clean.columns))\n",
        "print('\\nDTypes:')\n",
        "print(df_clean.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Carga (Load) — *Upsert* en BBDD\n",
        "La carga en producción se hace con `upsert_news_bulk(engine, df)`, que realiza un **upsert** por `url_hash` para evitar duplicados.\n",
        "\n",
        "En este cuaderno lo dejamos **opcional** para no requerir una BBDD real. Si defines `DO_LOAD=True` y tienes `DATABASE_URL`, se hará una inserción de prueba.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "DO_LOAD = False  # cambia a True si quieres probar la inserción en BBDD\n",
        "inserted = 0\n",
        "\n",
        "if DO_LOAD and DATABASE_URL:\n",
        "    try:\n",
        "        from src.repositories.db import init_engine\n",
        "        from src.repositories.news import upsert_news_bulk\n",
        "        engine = init_engine(DATABASE_URL)\n",
        "        inserted = upsert_news_bulk(engine, df_clean)\n",
        "        print('Filas insertadas/actualizadas:', inserted)\n",
        "    except Exception as e:\n",
        "        print('Fallo en carga:', str(e))\n",
        "else:\n",
        "    print('Carga desactivada o DATABASE_URL no disponible.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Métricas rápidas y validaciones\n",
        "- Duplicados por `url_hash` tras limpieza: deberían ser 0.\n",
        "- Porcentaje de registros filtrados por longitud mínima.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if df_clean is not None and not df_clean.empty:\n",
        "    dup = df_clean['url_hash'].duplicated().sum()\n",
        "    pct_filtered = None\n",
        "    if 'n_before' in globals() and n_before:\n",
        "        pct_filtered = round((n_before - n_after) / n_before * 100, 2)\n",
        "    print('Duplicados por url_hash:', dup)\n",
        "    print('Filtrados por longitud (%):', pct_filtered)\n",
        "else:\n",
        "    print('No hay datos limpios para calcular métricas.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Conclusiones y siguientes pasos\n**Lo aprendido / validado:**\n- La API devuelve campos suficientes para construir un `news` unificado.\n- La normalización corrige `author` vacíos, convierte fechas a UTC y deduplica por `url_hash`.\n- El filtro por longitud reduce ruido de *snippets* demasiado breves.\n\n**Mejoras posibles:**\n- Afinar el *scoring* de relevancia (ej. ponderar por fuente o presencia de palabras clave).\n- Añadir validaciones de URL (status 200 de `url_to_image`) si la latencia lo permite.\n- Enriquecer con NER/sentiment si el caso de uso lo requiere.\n\n**Pruebas reproducibles:**\n- El bloque de *Extract* puede activarse con `DO_EXTRACT=True` si hay `NEWSAPI_KEY`.\n- El bloque de *Load* puede activarse con `DO_LOAD=True` si hay `DATABASE_URL`.\n\n_Generado/actualizado: 2025-08-09 07:51 UTC_"
      ]
    }
  ]
}